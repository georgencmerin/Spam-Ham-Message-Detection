# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Lz6yq4FtOOv1re06reOD9qQjmiPZv1h
"""

# Commented out IPython magic to ensure Python compatibility.

import pandas as pd
import re
import string
import pprint

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import text
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
from sklearn.metrics import roc_auc_score, roc_curve, auc

from wordcloud import WordCloud

import matplotlib.pyplot as plt
import seaborn as sns

nltk.download("stopwords")
from collections import Counter

# %matplotlib inline

data = pd.read_csv("/content/drive/MyDrive/data's sem 3/spam.csv",encoding = "latin-1")#read data

data.columns#columns are viewed

cols = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']

data.drop(cols,axis=1,inplace=True)#drop null columns

data.duplicated().sum()#403 duplicated values are found in the data

df = data.drop_duplicates()#all duplicated data points are removed

df.describe()

df.columns = ["Spam/Ham","Messages"] #col names changed to spam ham

df['lable'] = df["Spam/Ham"].map({'ham': 0,'spam': 1})

!pip install transformers

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint


from tqdm.notebook import tqdm

import transformers

from tokenizers import BertWordPieceTokenizer

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

import numpy as np

def bert_encode(input_text, max_len):
    input_ids = []
    attension_masks = []
    for text in input_text:
        output_dict = tokenizer.encode_plus(
            text, 
            add_special_tokens = True,
            truncation=True,
            max_length = max_len,
            pad_to_max_length = True,
            return_attention_mask = True
        )
        input_ids.append(output_dict['input_ids'])
        attension_masks.append(output_dict['attention_mask'])
    return np.array(input_ids), np.array(attension_masks)

text = df['Messages']
target = df['lable']
train_input_ids, train_attention_masks = bert_encode(text, 60)

def create_model(bert_model):
    input_ids = tf.keras.Input(shape= (60,), dtype= 'int32')
    attention_masks = tf.keras.Input(shape= (60,), dtype= 'int32')
    #Now the model will take as input arrays of shape (None, 60)
    
    output = bert_model([input_ids, attention_masks])
    output = output[1]
    output = tf.keras.layers.Dense(32, activation= 'relu')(output)
    #and output arrays of shape (None, 32)
    output = tf.keras.layers.Dropout(0.2)(output)
    output = tf.keras.layers.Dense(1, activation= 'sigmoid')(output)
    #and output arrays of shape (None, 1)
    #As a generic step in the model building, we will now compile our model using adam as our optimizer and binary_crossentropy as our loss function. 
    #For metrics, we will use accuracy.
    model = tf.keras.models.Model(inputs= [input_ids, attention_masks], outputs= output)
    model.compile(Adam(lr=1e-5), loss= 'binary_crossentropy', metrics= ['accuracy'])
    return model

from transformers import TFBertModel
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

model = create_model(bert_model)
model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# history = model.fit(
#     [train_input_ids, train_attention_masks],
#     target, 
#     validation_split = 0.2,
#     epochs = 3,
#     batch_size = 10
# )

